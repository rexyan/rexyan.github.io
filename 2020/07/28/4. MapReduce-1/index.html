<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/favicon.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico">
  <link rel="mask-icon" href="/favicon.ico" color="#222">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.loli.net/css?family=Noto Serif SC:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"rexyan.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"top","display":"hide","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"buttons","active":null,"storage":true,"lazyload":true,"nav":{"disqus":{"text":"Load Disqus","order":-1}}},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":3,"unescape":false,"preload":false},"motion":{"enable":false,"async":true,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="MR 相关概念 Job(作业) :  一个MR程序称为一个Job MRAppMaster（MR任务的主节点）: 一个Job在运行时，会先启动一个进程，这个进程为 MRAppMaster。负责 Job 中执行状态的监控，容错，和 RM 申请资源，提交 Task 等。 Task(任务)：  Task是一个进程，负责某项计算。 Map(Map阶段): Map 是 MapReduce 程序运行的第一个阶段">
<meta name="keywords" content="Hadoop,MapReduce">
<meta property="og:type" content="article">
<meta property="og:title" content="4. MapReduce-1">
<meta property="og:url" content="https://rexyan.github.io/2020/07/28/4. MapReduce-1/index.html">
<meta property="og:site_name" content="星尘">
<meta property="og:description" content="MR 相关概念 Job(作业) :  一个MR程序称为一个Job MRAppMaster（MR任务的主节点）: 一个Job在运行时，会先启动一个进程，这个进程为 MRAppMaster。负责 Job 中执行状态的监控，容错，和 RM 申请资源，提交 Task 等。 Task(任务)：  Task是一个进程，负责某项计算。 Map(Map阶段): Map 是 MapReduce 程序运行的第一个阶段">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://r.photo.store.qq.com/psc?/V12EvAd609VbnF/ruAMsa53pVQWN7FLK88i5kP*h6unlPTbuijUhG0tIl4a8RpMOv5*ivHT7GQnD4hjQGkfk.fZbCq2Vm.dpLa3SZdwHFC8YXxKlkmUZmQGQfk!/mnull&bo=eAaUA3gGlAMDCSw!&rf=photolist&t=5/r/_yake_qzoneimgout.png">
<meta property="og:image" content="https://r.photo.store.qq.com/psc?/V12EvAd609VbnF/ruAMsa53pVQWN7FLK88i5kP*h6unlPTbuijUhG0tIl5OhBUMrUNO0AEQ0eWribXmZE6xbuejrsVH17vHP4.vOOZKh6a0JAQU3N*no7jbsI4!/mnull&bo=WgRGAVoERgEDCSw!&rf=photolist&t=5/r/_yake_qzoneimgout.png">
<meta property="og:image" content="https://r.photo.store.qq.com/psc?/V12EvAd609VbnF/ruAMsa53pVQWN7FLK88i5sp9MTygMMmxVsdI2JD4PSn4hjfk8v4Rdx3SMP8jNRrcBfrfnpR1uvoLP8VsO93m.j7dx8vhsFcFn9MPeG9NZl4!/mnull&bo=LQaAAnwGoAIDCV0!&rf=photolist&t=5/r/_yake_qzoneimgout.png">
<meta property="og:updated_time" content="2025-10-31T03:20:39.386Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="4. MapReduce-1">
<meta name="twitter:description" content="MR 相关概念 Job(作业) :  一个MR程序称为一个Job MRAppMaster（MR任务的主节点）: 一个Job在运行时，会先启动一个进程，这个进程为 MRAppMaster。负责 Job 中执行状态的监控，容错，和 RM 申请资源，提交 Task 等。 Task(任务)：  Task是一个进程，负责某项计算。 Map(Map阶段): Map 是 MapReduce 程序运行的第一个阶段">
<meta name="twitter:image" content="https://r.photo.store.qq.com/psc?/V12EvAd609VbnF/ruAMsa53pVQWN7FLK88i5kP*h6unlPTbuijUhG0tIl4a8RpMOv5*ivHT7GQnD4hjQGkfk.fZbCq2Vm.dpLa3SZdwHFC8YXxKlkmUZmQGQfk!/mnull&bo=eAaUA3gGlAMDCSw!&rf=photolist&t=5/r/_yake_qzoneimgout.png">

<link rel="canonical" href="https://rexyan.github.io/2020/07/28/4. MapReduce-1/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>4. MapReduce-1 | 星尘</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

  
  <link rel="stylesheet" href="https://cdn.staticfile.org/lxgw-wenkai-screen-webfont/1.6.0/lxgwwenkaiscreen.css" />
  <!-- 自定义为霞鹜文楷字体 -->
  <style>
	  body,div.post-body,h1,h2,h3,h4 {
		font-family: "LXGW WenKai Screen", sans-serif;
		font-size: 104%;
	  }
  </style>
  
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">星尘</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-读书">

    <a href="/books/" rel="section"><i class="address-book fa-fw"></i>读书</a>

  </li>
        <li class="menu-item menu-item-瞎扯">

    <a href="/crap/" rel="section"><i class="crap fa-fw"></i>瞎扯</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



<script src="https://cdn.jsdelivr.net/npm/echarts@4.8.0/dist/echarts.min.js"></script>

<meta name="referrer" content="never">




  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://rexyan.github.io/2020/07/28/4. MapReduce-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://raw.githubusercontent.com/rexyan/warehouse/master/20230809141242.jpg">
      <meta itemprop="name" content="Rex">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="星尘">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          4. MapReduce-1
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-07-28 23:41:57" itemprop="dateCreated datePublished" datetime="2020-07-28T23:41:57+00:00">2020-07-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-10-31 03:20:39" itemprop="dateModified" datetime="2025-10-31T03:20:39+00:00">2025-10-31</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Java-大数据进阶/" itemprop="url" rel="index"><span itemprop="name">Java 大数据进阶</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h3 id="MR-相关概念"><a href="#MR-相关概念" class="headerlink" title="MR 相关概念"></a>MR 相关概念</h3><ol>
<li>Job(作业) :  一个MR程序称为一个Job</li>
<li>MRAppMaster（MR任务的主节点）: 一个Job在运行时，会先启动一个进程，这个进程为 MRAppMaster。负责 Job 中执行状态的监控，容错，和 RM 申请资源，提交 Task 等。</li>
<li>Task(任务)：  Task是一个进程，负责某项计算。</li>
<li>Map(Map阶段): Map 是 MapReduce 程序运行的第一个阶段。Map阶段的目的是将输入的数据，进行切分。将一个大数据，切分为若干小部分。切分后，每个部分称为1片(split)，每片数据会交给一个Task（进程）进行计算，负责 Map 阶段的 Task 称为 MapTask。在一个 MR 程序的 Map 阶段，会启动N（取决于切片数，多少个切片就会启动多少个 MapTask）个 MapTask。每个 MapTask 是并行运行。</li>
<li>Reduce(Reduce阶段)： Reduce 是MapReduce 程序运行的第二个阶段(最后一个阶段)，Reduce 阶段的目的是将 Map 阶段，每个 MapTask 计算后的结果进行合并汇总，得到最终结果。Reduce阶段是可选的，不一定有。负责 Reduce 阶段的 Task 称为ReduceTask。一个Job可以通过设置，启动N个ReduceTask，这些ReduceTask也是并行运行，每个ReduceTask最终都会产生一个结果。</li>
</ol>
<h3 id="MR-相关组件"><a href="#MR-相关组件" class="headerlink" title="MR 相关组件"></a>MR 相关组件</h3><ol>
<li>Mapper:   map 阶段核心的处理逻辑</li>
<li>Reducer:   reduce 阶段核心的处理逻辑</li>
<li>InputFormat:  输入格式。MR 程序必须指定一个输入目录，一个输出目录，InputFormat 代表输入目录中文件的格式。如果是普通文件，可以使用FileInputFormat。如果是SequeceFile（hadoop提供的一种文件格式），可以使用 SequnceFileInputFormat，如果处理的数据在数据库中，需要使用 DBInputFormat。</li>
<li>RecordReader:  记录读取器。RecordReader 负责从输入格式中，读取数据，读取后封装为一组记录(k-v)。</li>
<li>OutPutFormat: 输出格式。OutPutFormat 代表 MR 处理后的结果，要以什么样的文件格式写出。将结果写出到一个普通文件中，可以使用 FileOutputFormat，将结果写出到数据库中，可以使用 DBOutPutFormat，将结果写出到 SequeceFil e中，可以使用 SequnceFileOutputFormat。</li>
<li>RecordWriter: 记录写出器。将处理的结果以什么样的格式写出到输出文件中。</li>
<li>Partitioner: 分区器。负责在 Mapper 将数据写出时，为每组 keyout-valueout 打上标记，进行分区。一个ReduceTask只会处理一个分区的数据。</li>
</ol>
<h3 id="MR-流程"><a href="#MR-流程" class="headerlink" title="MR 流程"></a>MR 流程</h3><ol>
<li>InputFormat 调用 RecordReader，从输入目录的文件中，读取一组数据，封装为 keyin-valuein 对象</li>
<li>将封装好的 key-value，交给 Mapper.map() ——&gt;将处理的结果写出 keyout-valueout</li>
<li>ReduceTask 启动 Reducer，使用 Reducer.reduce() 处理 Mapper写出的 keyout-valueout，</li>
<li>OutPutFormat 调用 RecordWriter，将 Reducer 处理后的 keyout-valueout 写出到文件</li>
</ol>
<p>Map阶段(MapTask)：  切片(Split) —– 读取数据(Read) —– 交给Mapper处理(Map) —– 分区和排序(sort)<br>Reduce阶段(ReduceTask):  拷贝数据(copy) —– 排序(sort) —– 合并(reduce) —– 写出(write)</p>
<h3 id="MR-编程"><a href="#MR-编程" class="headerlink" title="MR 编程"></a>MR 编程</h3><p>MR的编程只需要将自定义的组件和系统默认组件进行组合，组合之后运行即可，步骤：</p>
<ol>
<li>Map 阶段的核心处理逻辑需要编写在 Mapper 中</li>
<li>Reduce 阶段的核心处理逻辑需要编写在 Reducer 中</li>
<li>将编写的 Mapper 和 Reducer 进行组合，组合成一个 Job</li>
<li>对 Job 进行设置，设置后运行</li>
</ol>
<h4 id="wordcount"><a href="#wordcount" class="headerlink" title="wordcount"></a>wordcount</h4><p>InputFormat 的实现类很多</p>
<p><img src="https://r.photo.store.qq.com/psc?/V12EvAd609VbnF/ruAMsa53pVQWN7FLK88i5kP*h6unlPTbuijUhG0tIl4a8RpMOv5*ivHT7GQnD4hjQGkfk.fZbCq2Vm.dpLa3SZdwHFC8YXxKlkmUZmQGQfk!/mnull&amp;bo=eAaUA3gGlAMDCSw!&amp;rf=photolist&amp;t=5/r/_yake_qzoneimgout.png" alt></p>
<p>InputFormat 的作用：</p>
<ol>
<li>验证输入目录中的文件格式，是否符合当前 Job 的要求</li>
<li>生成切片，每个切片都会交给一个 MapTask 处理</li>
<li>提供 RecordReader，由 RecordReader 从切片中读取记录，交给 Mapper 处理</li>
</ol>
<p>InputFormat 中的 <code>List&lt;InputSplit&gt; getSplits</code> 方法的功能就是切片。<code>ecordReader&lt;K,V&gt; createRecordReader</code> 的功能是创建 RecordReader。默认 Hadoop 使用的是 TextInputFormat，而 TextInputFormat 创建的 RecordReader 是 LineRecordReader。所以 Hadoop 默认的 InputFormat 使用 TextInputFormat，默认是 Reader 使用 LineRecordReader。</p>
<h5 id="本地模式"><a href="#本地模式" class="headerlink" title="本地模式"></a>本地模式</h5><p>WCMapper 完整代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.yanrs.mr.wordcount;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * KEYIN, VALUEIN: mapper 输入的 key-value 类型，由当前 JOb 的 InputFormat的 RecordReader 决定</span></span><br><span class="line"><span class="comment"> * KEYOUT, VALUEOUT：mapper 输出的 key-value 类型</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WCMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Text outKey = <span class="keyword">new</span> Text();</span><br><span class="line">    <span class="keyword">private</span> IntWritable outValue = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// key 是行号，value 是一行的文本内容</span></span><br><span class="line">        System.out.println(<span class="string">"keyin: "</span> + key + <span class="string">" valuein: "</span> + value);</span><br><span class="line">        <span class="comment">// 将文本内容进行拆分，得到一个个单词组成的数组</span></span><br><span class="line">        String[] words = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">        <span class="comment">// 遍历数组，并输出，输出格式为（单词，1）</span></span><br><span class="line">        <span class="keyword">for</span> (String word:words) &#123;</span><br><span class="line">            outKey.set(word);</span><br><span class="line">            context.write(outKey, outValue);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>WCReducer 完整代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.yanrs.mr.wordcount;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * KEYIN,VALUEIN: Mapper 的输出做为这里的输入</span></span><br><span class="line"><span class="comment"> * KEYOUT,VALUEOUT： 自定义，因为这个 MR 程序是统计单词出现的频率，所以这里类型为 Text, IntWritable</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WCReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> IntWritable outValue = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//reduce 方法一次处理一组数据，key（单词） 相同的数据是一组</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 遍历每个 key（单词） ，让相同的 key（单词） 的值进行累加</span></span><br><span class="line">        <span class="keyword">for</span> (IntWritable value:values) &#123;</span><br><span class="line">            sum+=value.get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        outValue.set(sum);</span><br><span class="line">        <span class="comment">// 将结果写出，key 是单词，outValue 是累加的次数</span></span><br><span class="line">        context.write(key, outValue);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>WCDriver 完整代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.yanrs.mr.wordcount;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.URISyntaxException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 启动这个进程，那么就会运行该 job</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WCDriver</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException, URISyntaxException </span>&#123;</span><br><span class="line">        <span class="comment">// 获取文件系统</span></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://hadoop10:9000"</span>);</span><br><span class="line">        FileSystem fileSystem = FileSystem.get(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置输入目录和输出目录</span></span><br><span class="line">        Path inputPath = <span class="keyword">new</span> Path(<span class="string">"/wcinput"</span>);</span><br><span class="line">        Path outPath = <span class="keyword">new</span> Path(<span class="string">"/mroutput"</span>);</span><br><span class="line">        <span class="comment">// 输出目录存在就删除</span></span><br><span class="line">        <span class="keyword">if</span>(fileSystem.exists(outPath))&#123;</span><br><span class="line">            fileSystem.delete(outPath, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建 Job</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 job 名称</span></span><br><span class="line">        job.setJobName(<span class="string">"wordcount"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置job运行的 Mapper，Reducer</span></span><br><span class="line">        job.setMapperClass(WCMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(WCReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 Mapper，Reducer 的输出 key 和 value 类型。</span></span><br><span class="line">        <span class="comment">// job 需要根据 Mapper，Reducer 输出的 key value 类型准备序列化器，通过序列化器对输出的 key value 进行序列化和反序列化</span></span><br><span class="line">        <span class="comment">// 如果 Mapper，Reducer 输出的 key 和 value 类型一致，那么可以像下面一样直接设置 job 的最终输出类型</span></span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置输入输出目录</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, inputPath);</span><br><span class="line">        FileOutputFormat.setOutputPath(job, outPath);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 运行 Job 并打印日志信息</span></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>直接在 idea 中运行 WCDriver 的 main 方法即可。上面设置连接的是 Hadoop10 的文件系统，但是是在本地运行的。</p>
<p><a href="https://github.com/rexyan/roa/tree/master/mapreduce-test/src/main/java/com/yanrs/mr/wordcount" target="_blank" rel="noopener">代码地址</a></p>
<h5 id="yarn-上运行"><a href="#yarn-上运行" class="headerlink" title="yarn 上运行"></a>yarn 上运行</h5><p>WCMapper 完整代码，同上</p>
<p>WCReducer 完整代码，同上</p>
<p>在 yarn 上运行，需要指定运行方式为 yarn，且指定 resourcemanager 的地址</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 设置在 yarn 上运行</span></span><br><span class="line">conf.set(<span class="string">"mapreduce.framework.name"</span>, <span class="string">"yarn"</span>);</span><br><span class="line">conf.set(<span class="string">"yarn.resourcemanager.hostname"</span>, <span class="string">"hadoop11"</span>);</span><br></pre></td></tr></table></figure>
<p>还需要设置 job 所在的 jar 包</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// yarn 运行时候还需要设置 job 所在的 jar 包</span></span><br><span class="line">job.setJarByClass(WCDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">// 或者使用</span></span><br></pre></td></tr></table></figure>
<p>将代码打包，上传到 hadoop 上，使用 hadoop jar 命令运行</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar mapreduce-test-1.0-SNAPSHOT.jar com.yanrs.mr.wordcount.WCDriver</span><br></pre></td></tr></table></figure>
<p>WCDriver 完整代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.yanrs.mr.wordcount;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.URISyntaxException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 启动这个进程，那么就会运行该 job</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WCDriver</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException, URISyntaxException </span>&#123;</span><br><span class="line">        <span class="comment">// 获取文件系统</span></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://hadoop10:9000"</span>);</span><br><span class="line">        <span class="comment">// 设置在 yarn 上运行</span></span><br><span class="line">        conf.set(<span class="string">"mapreduce.framework.name"</span>, <span class="string">"yarn"</span>);</span><br><span class="line">        conf.set(<span class="string">"yarn.resourcemanager.hostname"</span>, <span class="string">"hadoop11"</span>);</span><br><span class="line"></span><br><span class="line">        FileSystem fileSystem = FileSystem.get(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置输入目录和输出目录</span></span><br><span class="line">        Path inputPath = <span class="keyword">new</span> Path(<span class="string">"/wcinput"</span>);</span><br><span class="line">        Path outPath = <span class="keyword">new</span> Path(<span class="string">"/mroutput"</span>);</span><br><span class="line">        <span class="comment">// 输出目录存在就删除</span></span><br><span class="line">        <span class="keyword">if</span>(fileSystem.exists(outPath))&#123;</span><br><span class="line">            fileSystem.delete(outPath, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建 Job</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// yarn 运行时候还需要设置 job 所在的 jar 包</span></span><br><span class="line">        job.setJarByClass(WCDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        <span class="comment">// 或者使用</span></span><br><span class="line">        <span class="comment">// job.setJar("mapreduce-test-1.0-SNAPSHOT.jar");</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 job 名称</span></span><br><span class="line">        job.setJobName(<span class="string">"wordcount"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置job运行的 Mapper，Reducer</span></span><br><span class="line">        job.setMapperClass(WCMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(WCReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 Mapper，Reducer 的输出 key 和 value 类型。</span></span><br><span class="line">        <span class="comment">// job 需要根据 Mapper，Reducer 输出的 key value 类型准备序列化器，通过序列化器对输出的 key value 进行序列化和反序列化</span></span><br><span class="line">        <span class="comment">// 如果 Mapper，Reducer 输出的 key 和 value 类型一致，那么可以像下面一样直接设置 job 的最终输出类型</span></span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置输入输出目录</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, inputPath);</span><br><span class="line">        FileOutputFormat.setOutputPath(job, outPath);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 运行 Job 并打印日志信息</span></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><a href="https://github.com/rexyan/roa/tree/master/mapreduce-test/src/main/java/com/yanrs/mr/wordcount" target="_blank" rel="noopener">代码地址</a></p>
<h4 id="自定义-Bean"><a href="#自定义-Bean" class="headerlink" title="自定义 Bean"></a>自定义 Bean</h4><p><img src="https://r.photo.store.qq.com/psc?/V12EvAd609VbnF/ruAMsa53pVQWN7FLK88i5kP*h6unlPTbuijUhG0tIl5OhBUMrUNO0AEQ0eWribXmZE6xbuejrsVH17vHP4.vOOZKh6a0JAQU3N*no7jbsI4!/mnull&amp;bo=WgRGAVoERgEDCSw!&amp;rf=photolist&amp;t=5/r/_yake_qzoneimgout.png" alt></p>
<p>数据格式如上所示，需要统计每个手机消耗的上行，下行，总流量信息</p>
<p>FlowBeanMapper 代码如下，mapper 输入参数 key 为行号，value 为一行的文本。mapper 输出参数 key 手机号，value 为 bean 对象（对象中分别有上行，下行，总流量三个属性）</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.yanrs.mr.flowbean;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * mapper 输入参数 key 为行号，value 为一行的文本</span></span><br><span class="line"><span class="comment"> * mapper 输出参数 key 手机号，value bean 对象（对象中分别有上行，下行，总流量三个属性）</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBeanMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt;</span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Text outKey = <span class="keyword">new</span> Text();</span><br><span class="line">    <span class="keyword">private</span> FlowBean flowBean = <span class="keyword">new</span> FlowBean();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// key 为序列号，value 为每行的内容</span></span><br><span class="line">        String[] words = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 封装手机号</span></span><br><span class="line">        outKey.set(words[<span class="number">1</span>]);</span><br><span class="line">        <span class="comment">// 上行流量</span></span><br><span class="line">        flowBean.setUpFlow(Long.parseLong(words[words.length - <span class="number">3</span>]));</span><br><span class="line">        <span class="comment">// 下行流量</span></span><br><span class="line">        flowBean.setDownFlow(Long.parseLong(words[words.length - <span class="number">2</span>]));</span><br><span class="line">        context.write(outKey, flowBean);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>FlowBean 为实体类，有三个属性，需要实现 hadoop 的序列化方法。需要重写 write（称为序列化） 和 readFields（称为反序列化） 方法。并且反序列化和序列化的顺序要一致，并且提供属性的 get，set 方法，空参构造，toString 方法。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.yanrs.mr.flowbean;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBean</span> <span class="keyword">implements</span> <span class="title">Writable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> upFlow;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> downFlow;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> sumFlow;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 序列化, 在写出属性时，如果属性为引用数据类型，那么属性不能为 null</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> dataOutput</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        dataOutput.writeLong(upFlow);</span><br><span class="line">        dataOutput.writeLong(downFlow);</span><br><span class="line">        dataOutput.writeLong(sumFlow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 反序列化，反序列化和序列化的顺序要一致</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> dataInput</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        upFlow = dataInput.readLong();</span><br><span class="line">        downFlow = dataInput.readLong();</span><br><span class="line">        sumFlow = dataInput.readLong();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getUpFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUpFlow</span><span class="params">(<span class="keyword">long</span> upFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getDownFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setDownFlow</span><span class="params">(<span class="keyword">long</span> downFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getSumFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSumFlow</span><span class="params">(<span class="keyword">long</span> sumFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"FlowBean&#123;"</span> +</span><br><span class="line">                <span class="string">"upFlow="</span> + upFlow +</span><br><span class="line">                <span class="string">", downFlow="</span> + downFlow +</span><br><span class="line">                <span class="string">", sumFlow="</span> + sumFlow +</span><br><span class="line">                <span class="string">'&#125;'</span>;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>FlowBeanReducer 处理 FlowBeanMapper 输出的数据，所以输入 key 和 value 的类型分别为 Text 和 FlowBean。输出也为 Text, FlowBean</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.yanrs.mr.flowbean;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 输入 key 和 value 的类型分别为 Text 和 FlowBean</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBeanReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span> &lt;<span class="title">Text</span>, <span class="title">FlowBean</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> FlowBean outValue = <span class="keyword">new</span> FlowBean();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;FlowBean&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 累加每个手机号的上行流量和下行流量，并计算总流量</span></span><br><span class="line">        <span class="keyword">long</span> sumUpFlow = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">long</span> sumDownFlow = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (FlowBean flowBean: values) &#123;</span><br><span class="line">            sumUpFlow += flowBean.getUpFlow();</span><br><span class="line">            sumDownFlow += flowBean.getDownFlow();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将值封装进入 FlowBean 中</span></span><br><span class="line">        outValue.setDownFlow(sumDownFlow);</span><br><span class="line">        outValue.setUpFlow(sumUpFlow);</span><br><span class="line">        outValue.setSumFlow(sumDownFlow + sumUpFlow);</span><br><span class="line"></span><br><span class="line">        context.write(key, outValue);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>FlowBeanDriver 中设置输入和输出目录，设置 MapperClass 和 ReducerClass。设置 Mapper，Reducer 的输出 key 和 value 类型。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.yanrs.mr.flowbean;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.yanrs.mr.wordcount.WCMapper;</span><br><span class="line"><span class="keyword">import</span> com.yanrs.mr.wordcount.WCReducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.URISyntaxException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 启动这个进程，那么就会运行该 job</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBeanDriver</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException, URISyntaxException </span>&#123;</span><br><span class="line">        <span class="comment">// 获取文件系统</span></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://hadoop10:9000"</span>);</span><br><span class="line"></span><br><span class="line">        FileSystem fileSystem = FileSystem.get(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置输入目录和输出目录</span></span><br><span class="line">        Path inputPath = <span class="keyword">new</span> Path(<span class="string">"/mrinput/flowbean"</span>);</span><br><span class="line">        Path outPath = <span class="keyword">new</span> Path(<span class="string">"/mroutput/flowbean"</span>);</span><br><span class="line">        <span class="comment">// 输出目录存在就删除</span></span><br><span class="line">        <span class="keyword">if</span>(fileSystem.exists(outPath))&#123;</span><br><span class="line">            fileSystem.delete(outPath, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建 Job</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 job 名称</span></span><br><span class="line">        job.setJobName(<span class="string">"FlowBean"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置job运行的 Mapper，Reducer</span></span><br><span class="line">        job.setMapperClass(FlowBeanMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(FlowBeanReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 Mapper，Reducer 的输出 key 和 value 类型。</span></span><br><span class="line">        <span class="comment">// job 需要根据 Mapper，Reducer 输出的 key value 类型准备序列化器，通过序列化器对输出的 key value 进行序列化和反序列化</span></span><br><span class="line">        <span class="comment">// 如果 Mapper，Reducer 输出的 key 和 value 类型一致，那么可以像下面一样直接设置 job 的最终输出类型</span></span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置输入输出目录</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, inputPath);</span><br><span class="line">        FileOutputFormat.setOutputPath(job, outPath);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 运行 Job 并打印日志信息</span></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>因为没有配置在 yarn 上运行，所以直接 idea 运行即可。结果如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">13470253144	FlowBean&#123;upFlow=180, downFlow=180, sumFlow=360&#125;</span><br><span class="line">13509468723	FlowBean&#123;upFlow=7335, downFlow=110349, sumFlow=117684&#125;</span><br><span class="line">13560439638	FlowBean&#123;upFlow=918, downFlow=4938, sumFlow=5856&#125;</span><br><span class="line">13568436656	FlowBean&#123;upFlow=3597, downFlow=25635, sumFlow=29232&#125;</span><br><span class="line">13590439668	FlowBean&#123;upFlow=1116, downFlow=954, sumFlow=2070&#125;</span><br><span class="line">13630577991	FlowBean&#123;upFlow=6960, downFlow=690, sumFlow=7650&#125;</span><br><span class="line">13682846555	FlowBean&#123;upFlow=1938, downFlow=2910, sumFlow=4848&#125;</span><br><span class="line">......</span><br></pre></td></tr></table></figure>
<p><a href="https://github.com/rexyan/roa/tree/master/mapreduce-test/src/main/java/com/yanrs/mr/flowbean" target="_blank" rel="noopener">代码地址</a></p>
<h4 id="默认的切片流程"><a href="#默认的切片流程" class="headerlink" title="默认的切片流程"></a>默认的切片流程</h4><h5 id="片和块的关系"><a href="#片和块的关系" class="headerlink" title="片和块的关系"></a>片和块的关系</h5><p><strong>片</strong>：在计算MR程序时，才会切片。在运行程序时，临时将文件从逻辑上划分为若干部分（所以只是逻辑上的切片，并不是真正的切分），使用的输入格式不同（不同的 InputFormat），切片的方式不同，切片的数量也不同。每片的数据最终也是以块的形式存储在 HDFS。</p>
<p><strong>块</strong>： 在向HDFS写文件时，文件中的内容以块为单位存储，块是实际的物理存在。</p>
<p>建议： 片大小最好等于块大小，将片大小设置和块大小一致，可以最大限度减少因为切片带来的磁盘IO和网络IO，MR计算框架速度慢的原因在于在执行MR时，会发生频繁的磁盘IO和网络IO。理论上来说：如果文件的数据量是一定的话，片越大，切片数量少，启动的 MapTask 少，Map 阶段运算慢，片越小，切片数量多，启动的MapTask多，Map阶段运算快。<strong>默认情况下片大小就是块大小</strong>，即文件的块大小默认为 128M，默认每片就是128M。<strong>MapTask的数量只取决于切片数，有多少切片就有多少个 MapTask</strong></p>
<p>如果需要调节片大小 &gt; 块大小：那么需要配置 <code>mapreduce.input.fileinputformat.split.minsize</code> &gt; 128M</p>
<p>如果需要调节片大小 &lt; 块大小：那么需要配置 <code>mapreduce.input.fileinputformat.split.maxsize</code> &lt; 128M</p>
<h5 id="FileInputFormat的切片策略-默认"><a href="#FileInputFormat的切片策略-默认" class="headerlink" title="FileInputFormat的切片策略(默认)"></a>FileInputFormat的切片策略(默认)</h5><ol>
<li>获取当前输入目录中所有的文件</li>
<li>以文件为单位切片，如果文件为空文件，默认创建一个空的切片</li>
<li>如果文件不为空，尝试判断文件是否可切(不是压缩文件，都可切)</li>
<li>如果文件不可切，整个文件作为1片</li>
<li>如果文件可切，先获取片大小(默认等于块大小)，循环判断 待切部分/ 片大小 &gt; 1.1倍，如果大于先切去一片，再判断…</li>
<li>剩余部分整个作为1片</li>
</ol>
<h3 id="常见的输入格式"><a href="#常见的输入格式" class="headerlink" title="常见的输入格式"></a>常见的输入格式</h3><p><img src="https://r.photo.store.qq.com/psc?/V12EvAd609VbnF/ruAMsa53pVQWN7FLK88i5sp9MTygMMmxVsdI2JD4PSn4hjfk8v4Rdx3SMP8jNRrcBfrfnpR1uvoLP8VsO93m.j7dx8vhsFcFn9MPeG9NZl4!/mnull&amp;bo=LQaAAnwGoAIDCV0!&amp;rf=photolist&amp;t=5/r/_yake_qzoneimgout.png" alt></p>
<p>FileInputFormat 中有六个子类，下面总结一下常见的四个子类的切片策略和 RecordReader</p>
<h4 id="TextInputFormat"><a href="#TextInputFormat" class="headerlink" title="TextInputFormat"></a>TextInputFormat</h4><p>TextInputFormat 常用于输入目录中全部是文本文件</p>
<p>切片策略：  默认的切片策略</p>
<p>RecordReader: LineRecordReader,一次处理一行，将一行内容的偏移量作为key，一行内容作为value，即 key 的类型为 LongWritable，value 的类型为 Text</p>
<p>上面的 wordcount 例子就是使用的默认的 TextInputFormat</p>
<h4 id="NlineInputFormat"><a href="#NlineInputFormat" class="headerlink" title="NlineInputFormat"></a>NlineInputFormat</h4><p>切片策略： 以文件为单位，读取配置中 <code>mapreduce.input.lineinputformat.linespermap</code> 参数（默认为1），每次这么多行切为一片。</p>
<p>RecordReader: LineRecordReader,一次处理一行，将一行内容的偏移量作为key，一行内容作为value，即 key 的类型为 LongWritable，value 的类型为 Text</p>
<p>NLMapper 完整代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.yanrs.mr.nline;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * KEYIN, VALUEIN: mapper 输入的 key-value 类型，由当前 JOb 的 InputFormat的 RecordReader 决定</span></span><br><span class="line"><span class="comment"> * KEYOUT, VALUEOUT：mapper 输出的 key-value 类型</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NLMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Text outKey = <span class="keyword">new</span> Text();</span><br><span class="line">    <span class="keyword">private</span> IntWritable outValue = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// key 是行号，value 是一行的文本内容</span></span><br><span class="line">        System.out.println(<span class="string">"keyin: "</span> + key + <span class="string">" valuein: "</span> + value);</span><br><span class="line">        <span class="comment">// 将文本内容进行拆分，得到一个个单词组成的数组</span></span><br><span class="line">        String[] words = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">        <span class="comment">// 遍历数组，并输出，输出格式为（单词，1）</span></span><br><span class="line">        <span class="keyword">for</span> (String word:words) &#123;</span><br><span class="line">            outKey.set(word);</span><br><span class="line">            context.write(outKey, outValue);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>NLReducer 完整代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.yanrs.mr.nline;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * KEYIN,VALUEIN: Mapper 的输出做为这里的输入</span></span><br><span class="line"><span class="comment"> * KEYOUT,VALUEOUT： 自定义，因为这个 MR 程序是统计单词出现的频率，所以这里类型为 Text, IntWritable</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NLReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> IntWritable outValue = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//reduce 方法一次处理一组数据，key（单词） 相同的数据是一组</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 遍历每个 key（单词） ，让相同的 key（单词） 的值进行累加</span></span><br><span class="line">        <span class="keyword">for</span> (IntWritable value:values) &#123;</span><br><span class="line">            sum+=value.get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        outValue.set(sum);</span><br><span class="line">        <span class="comment">// 将结果写出，key 是单词，outValue 是累加的次数</span></span><br><span class="line">        context.write(key, outValue);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>NLDriver</code> 完整代码。在 <code>Driver</code> 中新增设置使用 <code>NLineInputFormat</code>。默认是一行切分为一片，如果需要设置可以在 conf 中设置 <code>mapreduce.input.lineinputformat.linespermap</code> 值即可。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.yanrs.mr.nline;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.NLineInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.URISyntaxException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 启动这个进程，那么就会运行该 job</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NLDriver</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException, URISyntaxException </span>&#123;</span><br><span class="line">        <span class="comment">// 获取文件系统</span></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://hadoop10:9000"</span>);</span><br><span class="line">        <span class="comment">// 设置几行为一片，默认一行一片</span></span><br><span class="line">        <span class="comment">// conf.set("mapreduce.input.lineinputformat.linespermap", "2");</span></span><br><span class="line"></span><br><span class="line">        FileSystem fileSystem = FileSystem.get(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置输入目录和输出目录</span></span><br><span class="line">        Path inputPath = <span class="keyword">new</span> Path(<span class="string">"/mrinput/nline"</span>);</span><br><span class="line">        Path outPath = <span class="keyword">new</span> Path(<span class="string">"/mroutput/nline"</span>);</span><br><span class="line">        <span class="comment">// 输出目录存在就删除</span></span><br><span class="line">        <span class="keyword">if</span>(fileSystem.exists(outPath))&#123;</span><br><span class="line">            fileSystem.delete(outPath, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建 Job</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置使用 NLineInputFormat</span></span><br><span class="line">        job.setInputFormatClass(NLineInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 job 名称</span></span><br><span class="line">        job.setJobName(<span class="string">"nline"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置job运行的 Mapper，Reducer</span></span><br><span class="line">        job.setMapperClass(NLMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(NLReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 Mapper，Reducer 的输出 key 和 value 类型。</span></span><br><span class="line">        <span class="comment">// job 需要根据 Mapper，Reducer 输出的 key value 类型准备序列化器，通过序列化器对输出的 key value 进行序列化和反序列化</span></span><br><span class="line">        <span class="comment">// 如果 Mapper，Reducer 输出的 key 和 value 类型一致，那么可以像下面一样直接设置 job 的最终输出类型</span></span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置输入输出目录</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, inputPath);</span><br><span class="line">        FileOutputFormat.setOutputPath(job, outPath);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 运行 Job 并打印日志信息</span></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><a href="https://github.com/rexyan/roa/tree/master/mapreduce-test/src/main/java/com/yanrs/mr/nline" target="_blank" rel="noopener">代码地址</a></p>
<h4 id="KeyValueTextInputFormat"><a href="#KeyValueTextInputFormat" class="headerlink" title="KeyValueTextInputFormat"></a>KeyValueTextInputFormat</h4><p>针对文本文件，使用分割字符，将每一行分割为 key 和 value，如果没有找到分隔符，当前行的内容作为 key，value 为空串。默认分隔符为 <code>\t</code>，可以通过参数 <code>mapreduce.input.keyvaluelinerecordreader.key.value.separator</code> 指定。</p>
<p>切片策略：默认的切片策略</p>
<p>RecordReader ： key 和 value 的类型都是 Text</p>
<p>KVMapper 完整代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.yanrs.mr.keyvalue;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * KEYIN, VALUEIN: mapper 输入的 key-value 类型，由当前 JOb 的 InputFormat的 RecordReader 决定</span></span><br><span class="line"><span class="comment"> * KEYOUT, VALUEOUT：mapper 输出的 key-value 类型</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KVMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> IntWritable outValue = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Text key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// key 是 * 之前的姓名，value 是计数1</span></span><br><span class="line">        context.write(key, outValue);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>KVReducer 完整代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.yanrs.mr.keyvalue;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * KEYIN,VALUEIN: Mapper 的输出做为这里的输入</span></span><br><span class="line"><span class="comment"> * KEYOUT,VALUEOUT： 自定义，因为这个 MR 程序是统计单词出现的频率，所以这里类型为 Text, IntWritable</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KVReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> IntWritable outValue = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//reduce 方法一次处理一组数据，key（单词） 相同的数据是一组</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 遍历每个 key（单词） ，让相同的 key（单词） 的值进行累加</span></span><br><span class="line">        <span class="keyword">for</span> (IntWritable value:values) &#123;</span><br><span class="line">            sum+=value.get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        outValue.set(sum);</span><br><span class="line">        <span class="comment">// 将结果写出，key 是单词，outValue 是累加的次数</span></span><br><span class="line">        context.write(key, outValue);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>KVDriver 完整代码如下，需要设置使用 KeyValueTextInputFormat，并且需要设置分隔符，需要注意的是分隔符只是一个 byte 类型的数据，即便传入的是一个字符串，也只会读取第一个字符。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.yanrs.mr.keyvalue;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.KeyValueLineRecordReader;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.NLineInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.URISyntaxException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 启动这个进程，那么就会运行该 job</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KVDriver</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException, URISyntaxException </span>&#123;</span><br><span class="line">        <span class="comment">// 获取文件系统</span></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://hadoop10:9000"</span>);</span><br><span class="line">        <span class="comment">// 设置分隔符(需要注意的是分隔符只是一个 byte 类型的数据，即便传入的是一个字符串，也只会读取第一个字符)</span></span><br><span class="line">        conf.set(<span class="string">"mapreduce.input.keyvaluelinerecordreader.key.value.separator"</span>, <span class="string">"*"</span>);</span><br><span class="line"></span><br><span class="line">        FileSystem fileSystem = FileSystem.get(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置输入目录和输出目录</span></span><br><span class="line">        Path inputPath = <span class="keyword">new</span> Path(<span class="string">"/mrinput/keyvalue"</span>);</span><br><span class="line">        Path outPath = <span class="keyword">new</span> Path(<span class="string">"/mroutput/keyvalue"</span>);</span><br><span class="line">        <span class="comment">// 输出目录存在就删除</span></span><br><span class="line">        <span class="keyword">if</span>(fileSystem.exists(outPath))&#123;</span><br><span class="line">            fileSystem.delete(outPath, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建 Job</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置使用 KeyValueTextInputFormat</span></span><br><span class="line">        job.setInputFormatClass(KeyValueTextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 job 名称</span></span><br><span class="line">        job.setJobName(<span class="string">"keyvalue"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置job运行的 Mapper，Reducer</span></span><br><span class="line">        job.setMapperClass(KVMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(KVReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 Mapper，Reducer 的输出 key 和 value 类型。</span></span><br><span class="line">        <span class="comment">// job 需要根据 Mapper，Reducer 输出的 key value 类型准备序列化器，通过序列化器对输出的 key value 进行序列化和反序列化</span></span><br><span class="line">        <span class="comment">// 如果 Mapper，Reducer 输出的 key 和 value 类型一致，那么可以像下面一样直接设置 job 的最终输出类型</span></span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置输入输出目录</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, inputPath);</span><br><span class="line">        FileOutputFormat.setOutputPath(job, outPath);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 运行 Job 并打印日志信息</span></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><a href="https://github.com/rexyan/roa/tree/master/mapreduce-test/src/main/java/com/yanrs/mr/keyvalue" target="_blank" rel="noopener">代码地址</a></p>
<h4 id="CombineTextInputFormat"><a href="#CombineTextInputFormat" class="headerlink" title="CombineTextInputFormat"></a>CombineTextInputFormat</h4><p>改变了传统的切片方式。将多个小文件，划分到一个切片中，适合小文件过多的场景。</p>
<p>切片策略：  先确定片的最大值 maxSize，maxSize 通过参数 <code>mapreduce.input.fileinputformat.split.maxsize</code> 设置。流程是以文件为单位，将每个文件划分为若干 part，如果文件的待切部分的大小小于等于 maxSize, 则整个待切部分作为1个 part，如果文件的待切部分的大小大于 maxsize 但是小于等于 2<em> maxSize, 那么将整个待切部分均匀的切分为2个 part。如果文件的待切部分的大小大于 2</em> maxSize, 那么先切去 maxSize 大小，得到 1个 part，剩余待切部分继续判断</p>
<p>RecordReader: LineRecordReader,一次处理一行，将一行内容的偏移量作为 key，一行内容作为 value，即 key 的类型为 LongWritable，value 的类型为 Text</p>
<p>CMMapper 完整代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.yanrs.mr.combine;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * KEYIN, VALUEIN: mapper 输入的 key-value 类型，由当前 JOb 的 InputFormat的 RecordReader 决定</span></span><br><span class="line"><span class="comment"> * KEYOUT, VALUEOUT：mapper 输出的 key-value 类型</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CMMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Text outKey = <span class="keyword">new</span> Text();</span><br><span class="line">    <span class="keyword">private</span> IntWritable outValue = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// key 是行号，value 是一行的文本内容</span></span><br><span class="line">        System.out.println(<span class="string">"keyin: "</span> + key + <span class="string">" valuein: "</span> + value);</span><br><span class="line">        <span class="comment">// 将文本内容进行拆分，得到一个个单词组成的数组</span></span><br><span class="line">        String[] words = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">        <span class="comment">// 遍历数组，并输出，输出格式为（单词，1）</span></span><br><span class="line">        <span class="keyword">for</span> (String word:words) &#123;</span><br><span class="line">            outKey.set(word);</span><br><span class="line">            context.write(outKey, outValue);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>CMReducer 完整代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.yanrs.mr.combine;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * KEYIN,VALUEIN: Mapper 的输出做为这里的输入</span></span><br><span class="line"><span class="comment"> * KEYOUT,VALUEOUT： 自定义，因为这个 MR 程序是统计单词出现的频率，所以这里类型为 Text, IntWritable</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CMReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> IntWritable outValue = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//reduce 方法一次处理一组数据，key（单词） 相同的数据是一组</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 遍历每个 key（单词） ，让相同的 key（单词） 的值进行累加</span></span><br><span class="line">        <span class="keyword">for</span> (IntWritable value:values) &#123;</span><br><span class="line">            sum+=value.get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        outValue.set(sum);</span><br><span class="line">        <span class="comment">// 将结果写出，key 是单词，outValue 是累加的次数</span></span><br><span class="line">        context.write(key, outValue);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>CMDriver 完整代码。需要设置多大文件切为一片，设置使用 CombineTextInputFormat</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.yanrs.mr.combine;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.URISyntaxException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 启动这个进程，那么就会运行该 job</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CMDriver</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException, URISyntaxException </span>&#123;</span><br><span class="line">        <span class="comment">// 获取文件系统</span></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://hadoop10:9000"</span>);</span><br><span class="line">        <span class="comment">// 设置多大文件切为一片</span></span><br><span class="line">        conf.set(<span class="string">"mapreduce.input.fileinputformat.split.maxsize"</span>, <span class="string">"2048"</span>);</span><br><span class="line"></span><br><span class="line">        FileSystem fileSystem = FileSystem.get(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置输入目录和输出目录</span></span><br><span class="line">        Path inputPath = <span class="keyword">new</span> Path(<span class="string">"/mrinput/combine"</span>);</span><br><span class="line">        Path outPath = <span class="keyword">new</span> Path(<span class="string">"/mroutput/combine"</span>);</span><br><span class="line">        <span class="comment">// 输出目录存在就删除</span></span><br><span class="line">        <span class="keyword">if</span>(fileSystem.exists(outPath))&#123;</span><br><span class="line">            fileSystem.delete(outPath, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建 Job</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置使用 CombineTextInputFormat</span></span><br><span class="line">        job.setInputFormatClass(CombineTextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 job 名称</span></span><br><span class="line">        job.setJobName(<span class="string">"combine"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置job运行的 Mapper，Reducer</span></span><br><span class="line">        job.setMapperClass(CMMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(CMReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 Mapper，Reducer 的输出 key 和 value 类型。</span></span><br><span class="line">        <span class="comment">// job 需要根据 Mapper，Reducer 输出的 key value 类型准备序列化器，通过序列化器对输出的 key value 进行序列化和反序列化</span></span><br><span class="line">        <span class="comment">// 如果 Mapper，Reducer 输出的 key 和 value 类型一致，那么可以像下面一样直接设置 job 的最终输出类型</span></span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置输入输出目录</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, inputPath);</span><br><span class="line">        FileOutputFormat.setOutputPath(job, outPath);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 运行 Job 并打印日志信息</span></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><a href="https://github.com/rexyan/roa/tree/master/mapreduce-test/src/main/java/com/yanrs/mr/combine" target="_blank" rel="noopener">代码地址</a></p>
<h3 id="MR-核心阶段划分"><a href="#MR-核心阶段划分" class="headerlink" title="MR 核心阶段划分"></a>MR 核心阶段划分</h3><h4 id="MapTask-阶段"><a href="#MapTask-阶段" class="headerlink" title="MapTask 阶段"></a>MapTask 阶段</h4><ol>
<li>map</li>
<li>sort</li>
</ol>
<h4 id="RedcueTask-阶段"><a href="#RedcueTask-阶段" class="headerlink" title="RedcueTask 阶段"></a>RedcueTask 阶段</h4><ol start="3">
<li>copy</li>
<li>sort</li>
<li>reduce</li>
</ol>
<h4 id="shuffle-阶段"><a href="#shuffle-阶段" class="headerlink" title="shuffle 阶段"></a>shuffle 阶段</h4><p>上面的 2-4 又称为 shuffle 阶段。Shuffle 阶段横跨 MapTask 和 RedcueTask，在MapTask端也有 Shuffle，在RedcueTask 也有 Shuffle。具体 Shuffle 阶段指 MapTask 的 map 方法运行之后到 RedcuceTask 的 reduce 方法运行之前。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>mapper 的输出，为 reducer 的输入，mapper 的输出由不同的 InputFormat 的 RecordReader 决定。</p>
<p>不同的 InputFormat 有着不同的切片策略，默认如果不设置，那么使用的是 TextInputFormat。</p>
<p>reduce 方法一次处理一组数据，key 相同的数据为一组。</p>
<p>mapper 和 reducer 的输出数据格式由自己根据需求来设置，可以是 hadoop 内置的类型，也可以自定义 bean。</p>
<p>如果要将编写好的程序在 yarn 上运行，那么需要配置 yarn 的地址，设置 job 所在的 jar 包，将程序打包为 jar 之后运行。</p>

    </div>

    
    
    

      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/Hadoop/" rel="tag"><i class="fa fa-tag"></i> Hadoop</a>
              <a href="/tags/MapReduce/" rel="tag"><i class="fa fa-tag"></i> MapReduce</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/07/27/JSONP 单点登录方案/" rel="prev" title="JSONP 单点登录方案">
      <i class="fa fa-chevron-left"></i> JSONP 单点登录方案
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/08/20/5. MapReduce-2/" rel="next" title="5. MapReduce-2">
      5. MapReduce-2 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#MR-相关概念"><span class="nav-number">1.</span> <span class="nav-text">MR 相关概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MR-相关组件"><span class="nav-number">2.</span> <span class="nav-text">MR 相关组件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MR-流程"><span class="nav-number">3.</span> <span class="nav-text">MR 流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MR-编程"><span class="nav-number">4.</span> <span class="nav-text">MR 编程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#wordcount"><span class="nav-number">4.1.</span> <span class="nav-text">wordcount</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#本地模式"><span class="nav-number">4.1.1.</span> <span class="nav-text">本地模式</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#yarn-上运行"><span class="nav-number">4.1.2.</span> <span class="nav-text">yarn 上运行</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#自定义-Bean"><span class="nav-number">4.2.</span> <span class="nav-text">自定义 Bean</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#默认的切片流程"><span class="nav-number">4.3.</span> <span class="nav-text">默认的切片流程</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#片和块的关系"><span class="nav-number">4.3.1.</span> <span class="nav-text">片和块的关系</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#FileInputFormat的切片策略-默认"><span class="nav-number">4.3.2.</span> <span class="nav-text">FileInputFormat的切片策略(默认)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#常见的输入格式"><span class="nav-number">5.</span> <span class="nav-text">常见的输入格式</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#TextInputFormat"><span class="nav-number">5.1.</span> <span class="nav-text">TextInputFormat</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#NlineInputFormat"><span class="nav-number">5.2.</span> <span class="nav-text">NlineInputFormat</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#KeyValueTextInputFormat"><span class="nav-number">5.3.</span> <span class="nav-text">KeyValueTextInputFormat</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CombineTextInputFormat"><span class="nav-number">5.4.</span> <span class="nav-text">CombineTextInputFormat</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MR-核心阶段划分"><span class="nav-number">6.</span> <span class="nav-text">MR 核心阶段划分</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#MapTask-阶段"><span class="nav-number">6.1.</span> <span class="nav-text">MapTask 阶段</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RedcueTask-阶段"><span class="nav-number">6.2.</span> <span class="nav-text">RedcueTask 阶段</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#shuffle-阶段"><span class="nav-number">6.3.</span> <span class="nav-text">shuffle 阶段</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#总结"><span class="nav-number">7.</span> <span class="nav-text">总结</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Rex"
      src="https://raw.githubusercontent.com/rexyan/warehouse/master/20230809141242.jpg">
  <p class="site-author-name" itemprop="name">Rex</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">446</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">20</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">183</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2017 – 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa-hand-o-right"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Rex</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
<script src="/js/utils.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>



  




  <script src="/js/local-search.js"></script>












  

  

</body>
</html>
